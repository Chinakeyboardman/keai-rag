# æ¶æ„ä¼˜åŒ–å®æ–½æŒ‡å—

## ğŸš€ å¿«é€Ÿå¼€å§‹ï¼šæ¸è¿›å¼ä¼˜åŒ–

æœ¬æ–‡æ¡£æä¾›å…·ä½“çš„ä»£ç ç¤ºä¾‹å’Œå®æ–½æ­¥éª¤ï¼Œå¸®åŠ©æ‚¨é€æ­¥ä¼˜åŒ–æ¶æ„ã€‚

---

## é˜¶æ®µä¸€ï¼šå¼‚æ­¥åŒ–æ”¹é€ ï¼ˆæœ€å¿«è§æ•ˆï¼‰

### 1.1 å®‰è£…ä¾èµ–

```bash
pip install celery redis httpx aiohttp
```

### 1.2 åˆ›å»ºå¼‚æ­¥ä»»åŠ¡é˜Ÿåˆ—

#### `services/celery_app.py`
```python
from celery import Celery
from config.settings import settings

celery_app = Celery(
    'rag_tasks',
    broker=f'redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/0',
    backend=f'redis://{settings.REDIS_HOST}:{settings.REDIS_PORT}/0'
)

celery_app.conf.update(
    task_serializer='json',
    accept_content=['json'],
    result_serializer='json',
    timezone='UTC',
    enable_utc=True,
    task_track_started=True,
    task_time_limit=300,  # 5åˆ†é’Ÿè¶…æ—¶
    worker_prefetch_multiplier=1,  # é˜²æ­¢ä»»åŠ¡å †ç§¯
)
```

#### `services/tasks/document_tasks.py`
```python
from services.celery_app import celery_app
from src.processors.pdf_processor import PDFProcessor
from src.services.embedding_service import get_embedding_service
from src.core.vector_store_manager import get_vector_store_manager
from src.utils.logger import logger

@celery_app.task(bind=True, max_retries=3)
def process_document_task(self, document_id: str, file_path: str):
    """å¼‚æ­¥å¤„ç†æ–‡æ¡£ä¸Šä¼ """
    try:
        logger.info(f"å¼€å§‹å¤„ç†æ–‡æ¡£: {document_id}")
        
        # 1. PDFè§£æ
        processor = PDFProcessor()
        chunks = processor.process(file_path)
        logger.info(f"æ–‡æ¡£è§£æå®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
        
        # 2. å‘é‡åŒ–
        embedding_service = get_embedding_service()
        vectors = []
        texts = []
        metadatas = []
        
        for i, chunk in enumerate(chunks):
            vector = embedding_service.embed_text(chunk.text)
            vectors.append(vector)
            texts.append(chunk.text)
            metadatas.append({
                "document_id": document_id,
                "chunk_index": i,
                **chunk.metadata
            })
        
        logger.info(f"å‘é‡åŒ–å®Œæˆï¼Œå…± {len(vectors)} ä¸ªå‘é‡")
        
        # 3. å­˜å‚¨å‘é‡
        vector_store = get_vector_store_manager().get_store()
        ids = [f"{document_id}_chunk_{i}" for i in range(len(vectors))]
        success = vector_store.insert_vectors(
            vectors=vectors,
            texts=texts,
            metadatas=metadatas,
            ids=ids
        )
        
        if success:
            logger.info(f"æ–‡æ¡£å¤„ç†å®Œæˆ: {document_id}")
            return {"status": "success", "chunks_count": len(chunks)}
        else:
            raise Exception("å‘é‡å­˜å‚¨å¤±è´¥")
            
    except Exception as e:
        logger.error(f"æ–‡æ¡£å¤„ç†å¤±è´¥: {e}", exc_info=True)
        # é‡è¯•
        raise self.retry(exc=e, countdown=60)
```

### 1.3 ä¿®æ”¹æ–‡æ¡£ä¸Šä¼ æ¥å£

#### `src/api/routes/document.py`ï¼ˆä¿®æ”¹åï¼‰
```python
from fastapi import APIRouter, UploadFile, File, HTTPException
from services.tasks.document_tasks import process_document_task
from src.utils.logger import logger

router = APIRouter()

@router.post("/upload")
async def upload_document(file: UploadFile = File(...)):
    """ä¸Šä¼ æ–‡æ¡£ï¼ˆå¼‚æ­¥å¤„ç†ï¼‰"""
    import uuid
    import os
    from pathlib import Path
    
    # ä¿å­˜æ–‡ä»¶
    document_id = str(uuid.uuid4())
    file_extension = Path(file.filename).suffix
    file_path = Path(f"data/documents/{document_id}{file_extension}")
    file_path.parent.mkdir(parents=True, exist_ok=True)
    
    with open(file_path, "wb") as f:
        content = await file.read()
        f.write(content)
    
    # å¼‚æ­¥å¤„ç†
    task = process_document_task.delay(document_id, str(file_path))
    
    return {
        "document_id": document_id,
        "status": "processing",
        "task_id": task.id,
        "message": "æ–‡æ¡£å·²ä¸Šä¼ ï¼Œæ­£åœ¨å¤„ç†ä¸­"
    }

@router.get("/upload/status/{task_id}")
async def get_upload_status(task_id: str):
    """æŸ¥è¯¢æ–‡æ¡£å¤„ç†çŠ¶æ€"""
    from services.celery_app import celery_app
    
    task = celery_app.AsyncResult(task_id)
    
    if task.state == 'PENDING':
        response = {'status': 'pending', 'progress': 0}
    elif task.state == 'PROGRESS':
        response = {
            'status': 'processing',
            'progress': task.info.get('progress', 0)
        }
    elif task.state == 'SUCCESS':
        response = {
            'status': 'completed',
            'result': task.result
        }
    else:
        response = {
            'status': 'failed',
            'error': str(task.info)
        }
    
    return response
```

### 1.4 å¯åŠ¨Celery Worker

```bash
# å¯åŠ¨worker
celery -A services.celery_app worker --loglevel=info --concurrency=4

# å¯åŠ¨flowerï¼ˆç›‘æ§ç•Œé¢ï¼‰
celery -A services.celery_app flower --port=5555
```

---

## é˜¶æ®µäºŒï¼šæ‹†åˆ†EmbeddingæœåŠ¡

### 2.1 åˆ›å»ºç‹¬ç«‹çš„EmbeddingæœåŠ¡

#### `services/embedding/main.py`
```python
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from typing import List
import numpy as np
from src.services.embedding_service import get_embedding_service

app = FastAPI(title="Embedding Service")

class EmbedRequest(BaseModel):
    text: str

class EmbedBatchRequest(BaseModel):
    texts: List[str]
    batch_size: int = 32

class EmbedResponse(BaseModel):
    vector: List[float]
    dimension: int

class EmbedBatchResponse(BaseModel):
    vectors: List[List[float]]
    count: int

@app.post("/embed", response_model=EmbedResponse)
async def embed_text(request: EmbedRequest):
    """å•ä¸ªæ–‡æœ¬å‘é‡åŒ–"""
    try:
        embedding_service = get_embedding_service()
        vector = embedding_service.embed_text(request.text)
        
        return EmbedResponse(
            vector=vector.tolist(),
            dimension=len(vector)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/embed/batch", response_model=EmbedBatchResponse)
async def embed_batch(request: EmbedBatchRequest):
    """æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–"""
    try:
        embedding_service = get_embedding_service()
        vectors = embedding_service.embed_batch(
            request.texts,
            batch_size=request.batch_size
        )
        
        return EmbedBatchResponse(
            vectors=[v.tolist() for v in vectors],
            count=len(vectors)
        )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8003)
```

### 2.2 åˆ›å»ºEmbeddingæœåŠ¡å®¢æˆ·ç«¯

#### `src/clients/embedding_client.py`
```python
import httpx
from typing import List
import numpy as np
from config.settings import settings
from src.utils.logger import logger

class EmbeddingClient:
    """EmbeddingæœåŠ¡å®¢æˆ·ç«¯"""
    
    def __init__(self, base_url: str = None):
        self.base_url = base_url or getattr(
            settings, 'EMBEDDING_SERVICE_URL', 'http://localhost:8003'
        )
        self.client = httpx.AsyncClient(
            base_url=self.base_url,
            timeout=30.0,
            limits=httpx.Limits(max_connections=100, max_keepalive_connections=20)
        )
    
    async def embed_text(self, text: str) -> np.ndarray:
        """å‘é‡åŒ–å•ä¸ªæ–‡æœ¬"""
        try:
            response = await self.client.post(
                "/embed",
                json={"text": text}
            )
            response.raise_for_status()
            data = response.json()
            return np.array(data["vector"])
        except Exception as e:
            logger.error(f"EmbeddingæœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def embed_batch(self, texts: List[str], batch_size: int = 32) -> List[np.ndarray]:
        """æ‰¹é‡å‘é‡åŒ–"""
        try:
            response = await self.client.post(
                "/embed/batch",
                json={"texts": texts, "batch_size": batch_size}
            )
            response.raise_for_status()
            data = response.json()
            return [np.array(v) for v in data["vectors"]]
        except Exception as e:
            logger.error(f"Embeddingæ‰¹é‡æœåŠ¡è°ƒç”¨å¤±è´¥: {e}")
            raise
    
    async def close(self):
        """å…³é—­å®¢æˆ·ç«¯"""
        await self.client.aclose()

# å…¨å±€å®¢æˆ·ç«¯å®ä¾‹
_embedding_client = None

def get_embedding_client() -> EmbeddingClient:
    """è·å–Embeddingå®¢æˆ·ç«¯ï¼ˆå•ä¾‹ï¼‰"""
    global _embedding_client
    if _embedding_client is None:
        _embedding_client = EmbeddingClient()
    return _embedding_client
```

### 2.3 ä¿®æ”¹åŸæœ‰ä»£ç ä½¿ç”¨å®¢æˆ·ç«¯

#### `src/services/embedding_service.py`ï¼ˆä¿®æ”¹ï¼‰
```python
# æ·»åŠ è¿œç¨‹è°ƒç”¨æ”¯æŒ
async def embed_text_remote(text: str) -> np.ndarray:
    """ä½¿ç”¨è¿œç¨‹EmbeddingæœåŠ¡"""
    from src.clients.embedding_client import get_embedding_client
    client = get_embedding_client()
    return await client.embed_text(text)
```

---

## é˜¶æ®µä¸‰ï¼šæ‹†åˆ†LLMæœåŠ¡

### 3.1 åˆ›å»ºç‹¬ç«‹çš„LLMæœåŠ¡

#### `services/llm/main.py`
```python
from fastapi import FastAPI, HTTPException
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import List, Optional
from src.services.llm_service import get_llm_service
import json

app = FastAPI(title="LLM Service")

class GenerateRequest(BaseModel):
    prompt: str
    max_tokens: int = 512
    temperature: float = 0.7
    stream: bool = False

class GenerateResponse(BaseModel):
    text: str
    tokens_used: int

@app.post("/generate", response_model=GenerateResponse)
async def generate(request: GenerateRequest):
    """ç”Ÿæˆæ–‡æœ¬"""
    try:
        llm_service = get_llm_service()
        
        if request.stream:
            # æµå¼è¾“å‡º
            async def stream_generator():
                async for chunk in llm_service.generate_stream(
                    request.prompt,
                    max_tokens=request.max_tokens,
                    temperature=request.temperature
                ):
                    yield f"data: {json.dumps({'chunk': chunk})}\n\n"
            
            return StreamingResponse(
                stream_generator(),
                media_type="text/event-stream"
            )
        else:
            # éæµå¼
            result = await llm_service.generate_async(
                request.prompt,
                max_tokens=request.max_tokens,
                temperature=request.temperature
            )
            
            return GenerateResponse(
                text=result["text"],
                tokens_used=result.get("tokens_used", 0)
            )
    except Exception as e:
        raise HTTPException(status_code=500, detail=str(e))

@app.get("/health")
async def health():
    """å¥åº·æ£€æŸ¥"""
    return {"status": "healthy"}

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8004)
```

---

## é˜¶æ®µå››ï¼šDockeråŒ–

### 4.1 åˆ›å»ºDockerfile

#### `Dockerfile.api`
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8000

# å¯åŠ¨å‘½ä»¤
CMD ["uvicorn", "src.api.main:app", "--host", "0.0.0.0", "--port", "8000"]
```

#### `Dockerfile.embedding`
```dockerfile
FROM python:3.9-slim

WORKDIR /app

# å®‰è£…ä¾èµ–
COPY requirements.txt .
RUN pip install --no-cache-dir -r requirements.txt

# å¤åˆ¶ä»£ç 
COPY . .

# æš´éœ²ç«¯å£
EXPOSE 8003

# å¯åŠ¨å‘½ä»¤
CMD ["python", "services/embedding/main.py"]
```

### 4.2 åˆ›å»ºdocker-compose.yml

```yaml
version: '3.8'

services:
  # Redisï¼ˆæ¶ˆæ¯é˜Ÿåˆ—ï¼‰
  redis:
    image: redis:alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data

  # Qdrantï¼ˆå‘é‡æ•°æ®åº“ï¼‰
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
    volumes:
      - qdrant_data:/qdrant/storage

  # EmbeddingæœåŠ¡ï¼ˆ3ä¸ªå®ä¾‹ï¼‰
  embedding-service-1:
    build:
      context: .
      dockerfile: Dockerfile.embedding
    ports:
      - "8003:8003"
    environment:
      - EMBEDDING_MODEL_PATH=/models/embedding
    volumes:
      - ./models/embedding:/models/embedding

  embedding-service-2:
    build:
      context: .
      dockerfile: Dockerfile.embedding
    ports:
      - "8004:8003"
    environment:
      - EMBEDDING_MODEL_PATH=/models/embedding
    volumes:
      - ./models/embedding:/models/embedding

  embedding-service-3:
    build:
      context: .
      dockerfile: Dockerfile.embedding
    ports:
      - "8005:8003"
    environment:
      - EMBEDDING_MODEL_PATH=/models/embedding
    volumes:
      - ./models/embedding:/models/embedding

  # APIæœåŠ¡
  api:
    build:
      context: .
      dockerfile: Dockerfile.api
    ports:
      - "8000:8000"
    environment:
      - REDIS_URL=redis://redis:6379
      - QDRANT_URL=http://qdrant:6333
      - EMBEDDING_SERVICE_URL=http://embedding-service-1:8003
    depends_on:
      - redis
      - qdrant
      - embedding-service-1

  # Nginxï¼ˆè´Ÿè½½å‡è¡¡ï¼‰
  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx.conf:/etc/nginx/nginx.conf
    depends_on:
      - embedding-service-1
      - embedding-service-2
      - embedding-service-3

volumes:
  redis_data:
  qdrant_data:
```

### 4.3 Nginxé…ç½®

#### `nginx.conf`
```nginx
upstream embedding_service {
    least_conn;
    server embedding-service-1:8003;
    server embedding-service-2:8003;
    server embedding-service-3:8003;
}

server {
    listen 80;
    
    location /api/v1/embedding/ {
        proxy_pass http://embedding_service;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
    
    location /api/v1/ {
        proxy_pass http://api:8000;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
    }
}
```

---

## é˜¶æ®µäº”ï¼šç›‘æ§é›†æˆ

### 5.1 æ·»åŠ PrometheusæŒ‡æ ‡

#### `src/utils/metrics.py`
```python
from prometheus_client import Counter, Histogram, Gauge, generate_latest
from fastapi import Response

# è¯·æ±‚è®¡æ•°
request_count = Counter(
    'http_requests_total',
    'Total HTTP requests',
    ['method', 'endpoint', 'status']
)

# å“åº”æ—¶é—´
request_duration = Histogram(
    'http_request_duration_seconds',
    'HTTP request duration',
    ['method', 'endpoint']
)

# æ´»è·ƒè¿æ¥æ•°
active_connections = Gauge(
    'active_connections',
    'Number of active connections'
)

# å‘é‡åŒ–è¯·æ±‚æ•°
embedding_requests = Counter(
    'embedding_requests_total',
    'Total embedding requests',
    ['status']
)

# LLMè¯·æ±‚æ•°
llm_requests = Counter(
    'llm_requests_total',
    'Total LLM requests',
    ['status']
)
```

#### `src/api/main.py`ï¼ˆæ·»åŠ ä¸­é—´ä»¶ï¼‰
```python
from src.utils.metrics import request_count, request_duration
import time

@app.middleware("http")
async def metrics_middleware(request: Request, call_next):
    start_time = time.time()
    response = await call_next(request)
    
    # è®°å½•æŒ‡æ ‡
    duration = time.time() - start_time
    request_duration.labels(
        method=request.method,
        endpoint=request.url.path
    ).observe(duration)
    
    request_count.labels(
        method=request.method,
        endpoint=request.url.path,
        status=response.status_code
    ).inc()
    
    return response

@app.get("/metrics")
async def metrics():
    """PrometheusæŒ‡æ ‡ç«¯ç‚¹"""
    from src.utils.metrics import generate_latest
    return Response(content=generate_latest(), media_type="text/plain")
```

---

## ğŸ“Š æ€§èƒ½æµ‹è¯•

### ä½¿ç”¨Locustè¿›è¡Œå‹åŠ›æµ‹è¯•

#### `tests/load_test.py`
```python
from locust import HttpUser, task, between

class RAGUser(HttpUser):
    wait_time = between(1, 3)
    
    @task(3)
    def query(self):
        """æŸ¥è¯¢ä»»åŠ¡"""
        self.client.post(
            "/api/v1/query",
            json={"query": "æµ‹è¯•é—®é¢˜"}
        )
    
    @task(1)
    def upload(self):
        """ä¸Šä¼ ä»»åŠ¡"""
        with open("test.pdf", "rb") as f:
            self.client.post(
                "/api/v1/documents/upload",
                files={"file": f}
            )
```

è¿è¡Œæµ‹è¯•ï¼š
```bash
locust -f tests/load_test.py --host=http://localhost:8000
```

---

## ğŸ¯ å®æ–½æ£€æŸ¥æ¸…å•

### é˜¶æ®µä¸€ï¼šå¼‚æ­¥åŒ–
- [ ] å®‰è£…Celeryå’ŒRedis
- [ ] åˆ›å»ºå¼‚æ­¥ä»»åŠ¡
- [ ] ä¿®æ”¹ä¸Šä¼ æ¥å£
- [ ] å¯åŠ¨Celery Worker
- [ ] æµ‹è¯•å¼‚æ­¥å¤„ç†

### é˜¶æ®µäºŒï¼šæœåŠ¡æ‹†åˆ†
- [ ] åˆ›å»ºEmbeddingæœåŠ¡
- [ ] åˆ›å»ºEmbeddingå®¢æˆ·ç«¯
- [ ] ä¿®æ”¹ä»£ç ä½¿ç”¨å®¢æˆ·ç«¯
- [ ] æµ‹è¯•æœåŠ¡è°ƒç”¨

### é˜¶æ®µä¸‰ï¼šå®¹å™¨åŒ–
- [ ] åˆ›å»ºDockerfile
- [ ] åˆ›å»ºdocker-compose.yml
- [ ] é…ç½®Nginxè´Ÿè½½å‡è¡¡
- [ ] æµ‹è¯•å®¹å™¨éƒ¨ç½²

### é˜¶æ®µå››ï¼šç›‘æ§
- [ ] é›†æˆPrometheus
- [ ] é…ç½®Grafana
- [ ] æ·»åŠ å‘Šè­¦è§„åˆ™
- [ ] æµ‹è¯•ç›‘æ§ç³»ç»Ÿ

---

## ğŸ“š ä¸‹ä¸€æ­¥

1. **é€‰æ‹©å®æ–½é˜¶æ®µ**ï¼šæ ¹æ®å®é™…æƒ…å†µé€‰æ‹©ä»å“ªä¸ªé˜¶æ®µå¼€å§‹
2. **å‡†å¤‡ç¯å¢ƒ**ï¼šå®‰è£…å¿…è¦çš„ä¾èµ–å’ŒæœåŠ¡
3. **é€æ­¥å®æ–½**ï¼šæŒ‰ç…§æ£€æŸ¥æ¸…å•é€æ­¥å®Œæˆ
4. **æµ‹è¯•éªŒè¯**ï¼šæ¯ä¸ªé˜¶æ®µå®Œæˆåè¿›è¡Œæµ‹è¯•
5. **ç›‘æ§ä¼˜åŒ–**ï¼šæŒç»­ç›‘æ§å’Œä¼˜åŒ–æ€§èƒ½

---

*æœ€åæ›´æ–°æ—¶é—´ï¼š2026-01-02*

