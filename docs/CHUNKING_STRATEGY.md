# 文本切片策略详解

## 📋 概述

文本切片（Chunking）是RAG系统的核心环节，直接影响检索质量和答案准确性。本文档详细说明本项目的切片策略、参数配置和最佳实践。

---

## 🎯 切片策略

### 核心思想

1. **固定大小切片**：每个文本块的目标大小固定（`chunk_size`）
2. **重叠切片**：相邻文本块之间有重叠（`chunk_overlap`），保持上下文连续性
3. **智能分割**：优先在语义边界（标点符号、换行符）处分割，避免截断句子

### 默认参数

```python
chunk_size = 1000      # 每个块的目标大小（字符数）
chunk_overlap = 200    # 相邻块之间的重叠大小（字符数）
```

---

## ✂️ 分割算法

### 分割优先级

系统按以下优先级寻找分割点：

1. **段落分隔符**：`\n\n`（双换行符）
2. **换行符**：`\n`（单换行符）
3. **中文标点**：`。`、`！`、`？`
4. **英文标点**：`.`、`!`、`?`

### 分割流程

```
1. 从位置 start 开始，计算目标结束位置 end = start + chunk_size

2. 如果 end < 文本总长度：
   - 在 [start, end] 范围内，从后往前查找分割字符
   - 优先查找段落分隔符，然后是换行符，最后是标点符号
   - 找到后，将 end 调整到分割字符之后

3. 提取文本块：text[start:end]

4. 计算下一个块的起始位置：
   - new_start = end - chunk_overlap（考虑重叠）
   - 确保 new_start > start（防止死循环）

5. 重复步骤1-4，直到处理完整个文本
```

### 代码实现

```python
def split_text(self, text: str) -> List[str]:
    """分割文本为块"""
    chunks = []
    start = 0
    text_length = len(text)
    
    while start < text_length:
        # 计算结束位置
        end = min(start + self.chunk_size, text_length)
        
        # 如果不是最后一块，尝试在合适的位置分割
        if end < text_length:
            # 寻找分割字符（按优先级）
            split_chars = ['\n\n', '\n', '。', '！', '？', '.', '!', '?']
            best_split = end
            
            for char in split_chars:
                pos = text.rfind(char, start, end)
                if pos != -1:
                    best_split = pos + len(char)
                    break
            
            end = best_split
        
        # 提取块
        chunk = text[start:end].strip()
        if chunk:
            chunks.append(chunk)
        
        # 更新起始位置（考虑重叠）
        new_start = end - self.chunk_overlap if end < text_length else text_length
        start = new_start
    
    return chunks
```

---

## ⚙️ 参数配置

### 环境变量配置

在 `.env` 文件中配置：

```bash
# 文本分块大小（字符数）
CHUNK_SIZE=1000

# 文本分块重叠大小（字符数）
CHUNK_OVERLAP=200
```

### 代码中配置

```python
from src.processors.pdf_processor import PDFProcessor

# 使用自定义参数
processor = PDFProcessor(
    chunk_size=1000,
    chunk_overlap=200
)
```

---

## 📊 参数选择指南

### chunk_size（块大小）

**影响因素**：
- **太小**（<500）：上下文信息不足，可能丢失重要信息
- **太大**（>2000）：检索精度下降，包含无关信息

**推荐值**：
- **小文档**（<10页）：`500-800`
- **中等文档**（10-50页）：`1000-1500`（默认1000）
- **大文档**（>50页）：`1500-2000`

### chunk_overlap（重叠大小）

**作用**：
- 保持上下文连续性
- 避免重要信息被分割在两个块之间

**推荐值**：
- 通常为 `chunk_size` 的 15-25%
- 默认：`200`（chunk_size=1000的20%）

**示例**：
```python
chunk_size = 1000  →  chunk_overlap = 200  (20%)
chunk_size = 1500  →  chunk_overlap = 300  (20%)
chunk_size = 500   →  chunk_overlap = 100  (20%)
```

---

## 🔍 切片示例

### 示例1：标准切片

**原文**（假设长度2000字符）：
```
这是第一段内容。这是第二段内容。
这是第三段内容。这是第四段内容。
...
```

**切片结果**（chunk_size=1000, chunk_overlap=200）：
```
块1: [0-1000]     "这是第一段内容。这是第二段内容。这是第三段..."
块2: [800-1800]   "...这是第三段内容。这是第四段内容。这是第五段..."
块3: [1600-2000]  "...这是第五段内容。这是第六段内容。"
```

**特点**：
- 块1和块2有200字符重叠
- 块2和块3有200字符重叠
- 分割点在句号处，保持语义完整

### 示例2：智能分割

**原文**：
```
第一段内容。

第二段内容。

第三段内容。
```

**切片结果**：
```
块1: "第一段内容。\n\n第二段内容。"
块2: "第二段内容。\n\n第三段内容。"
```

**说明**：
- 优先在段落分隔符（`\n\n`）处分割
- 保持段落完整性

---

## 🎨 不同场景的配置建议

### 场景1：技术文档

**特点**：代码、公式、结构化内容多

**推荐配置**：
```bash
CHUNK_SIZE=800
CHUNK_OVERLAP=150
```

**原因**：技术文档通常需要更精确的上下文

### 场景2：长篇小说

**特点**：连续性强，段落长

**推荐配置**：
```bash
CHUNK_SIZE=1500
CHUNK_OVERLAP=300
```

**原因**：需要更大的块来保持故事连贯性

### 场景3：问答对文档

**特点**：问答结构清晰

**推荐配置**：
```bash
CHUNK_SIZE=500
CHUNK_OVERLAP=100
```

**原因**：每个问答对通常较短，小块更精确

### 场景4：学术论文

**特点**：章节结构清晰，段落较长

**推荐配置**：
```bash
CHUNK_SIZE=1200
CHUNK_OVERLAP=250
```

**原因**：需要保持章节和段落的完整性

---

## 🔧 高级配置

### 自定义分割字符

如果需要支持其他分割字符，可以修改 `src/processors/base.py`：

```python
# 在 split_text 方法中修改 split_chars
split_chars = [
    '\n\n',      # 段落分隔符
    '\n',        # 换行符
    '。', '！', '？',  # 中文标点
    '.', '!', '?',    # 英文标点
    '；', ';',        # 分号（可选）
    '，', ',',        # 逗号（可选，谨慎使用）
]
```

### 基于语义的分割（未来优化）

当前版本使用基于字符的分割，未来可以考虑：

1. **基于句子分割**：使用NLP工具识别句子边界
2. **基于段落分割**：识别段落结构
3. **基于主题分割**：使用主题模型识别主题边界

---

## 📈 性能影响

### 块数量

**计算公式**：
```
块数量 ≈ (文本总长度 - chunk_overlap) / (chunk_size - chunk_overlap)
```

**示例**：
- 文本长度：10000字符
- chunk_size：1000
- chunk_overlap：200
- 块数量 ≈ (10000 - 200) / (1000 - 200) ≈ 12块

### 存储空间

**向量存储**：
- 每个块生成一个向量（维度取决于Embedding模型）
- 块数量越多，存储空间越大

**检索性能**：
- 块数量越多，检索时间越长
- 需要在精度和性能之间平衡

---

## 🐛 常见问题

### Q1: 为什么有些块的实际大小小于chunk_size？

**A**: 因为系统优先在语义边界（标点符号、换行符）处分割，如果找不到合适的分割点，可能会产生较小的块。这是正常的，目的是保持语义完整性。

### Q2: 重叠大小如何影响检索结果？

**A**: 重叠可以：
- ✅ 提高检索召回率（重要信息不会因为被分割而丢失）
- ✅ 保持上下文连续性
- ⚠️ 增加存储空间和检索时间

### Q3: 如何选择最佳的chunk_size？

**A**: 建议：
1. 从默认值（1000）开始
2. 根据实际文档特点调整
3. 通过A/B测试比较检索效果
4. 考虑Embedding模型的上下文窗口限制

### Q4: 可以按页面分割吗？

**A**: 当前版本不支持按页面分割，但可以：
1. 修改 `PDFProcessor.extract_text()` 方法，按页面提取
2. 为每个页面单独调用 `split_text()`
3. 在元数据中记录页面信息

---

## 🔮 未来优化方向

1. **语义感知分割**：使用NLP模型识别语义边界
2. **自适应块大小**：根据文档类型自动调整参数
3. **层次化分割**：支持章节、段落、句子多级分割
4. **内容感知分割**：识别表格、代码块等特殊内容

---

## 📚 参考资源

- [LangChain Text Splitters](https://python.langchain.com/docs/modules/data_connection/document_transformers/)
- [RAG最佳实践](https://www.pinecone.io/learn/chunking-strategies/)
- [文本分割策略研究](https://arxiv.org/abs/2307.06364)

---

*最后更新时间：2026-01-02*

