#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
FAISS å‘é‡å­˜å‚¨å®ç°
ä½¿ç”¨ FAISS ä½œä¸ºæœ¬åœ°å‘é‡æ•°æ®åº“ï¼ˆé™çº§æ–¹æ¡ˆï¼‰
"""

import os
import json
import pickle
from pathlib import Path
from typing import List, Dict, Any, Optional
import numpy as np
import faiss

from .vector_store import BaseVectorStore, VectorSearchResult


class FAISSStore(BaseVectorStore):
    """FAISS å‘é‡å­˜å‚¨å®ç°"""
    
    def __init__(
        self,
        collection_name: str,
        dimension: int,
        storage_dir: str = "./data/vectors"
    ):
        """
        åˆå§‹åŒ– FAISS å­˜å‚¨
        
        Args:
            collection_name: é›†åˆåç§°
            dimension: å‘é‡ç»´åº¦
            storage_dir: å­˜å‚¨ç›®å½•
        """
        super().__init__(collection_name, dimension)
        self.storage_dir = Path(storage_dir)
        self.storage_dir.mkdir(parents=True, exist_ok=True)
        
        # æ–‡ä»¶è·¯å¾„
        self.index_path = self.storage_dir / f"{collection_name}.index"
        self.metadata_path = self.storage_dir / f"{collection_name}_metadata.pkl"
        self.config_path = self.storage_dir / f"{collection_name}_config.json"
        
        # åˆå§‹åŒ–ç´¢å¼•å’Œå…ƒæ•°æ®
        self.index: Optional[faiss.Index] = None
        self.metadata_store: Dict[int, Dict[str, Any]] = {}
        self.id_to_idx: Dict[str, int] = {}
        self.idx_to_id: Dict[int, str] = {}
        self.next_idx = 0
        
        # åŠ è½½æˆ–åˆ›å»ºç´¢å¼•
        if self.collection_exists():
            self._load()
        else:
            self.create_collection()
    
    def create_collection(self) -> bool:
        """
        åˆ›å»ºé›†åˆï¼ˆåˆ›å»º FAISS ç´¢å¼•ï¼‰
        
        Returns:
            æ˜¯å¦åˆ›å»ºæˆåŠŸ
        """
        try:
            # åˆ›å»º L2 è·ç¦»çš„ FAISS ç´¢å¼•
            self.index = faiss.IndexFlatL2(self.dimension)
            
            # ä¿å­˜é…ç½®
            config = {
                "collection_name": self.collection_name,
                "dimension": self.dimension,
                "index_type": "IndexFlatL2"
            }
            with open(self.config_path, 'w', encoding='utf-8') as f:
                json.dump(config, f, indent=2, ensure_ascii=False)
            
            # ä¿å­˜ç©ºç´¢å¼•
            self._save()
            
            return True
        except Exception as e:
            print(f"åˆ›å»º FAISS é›†åˆå¤±è´¥: {e}")
            return False
    
    def collection_exists(self) -> bool:
        """
        æ£€æŸ¥é›†åˆæ˜¯å¦å­˜åœ¨
        
        Returns:
            é›†åˆæ˜¯å¦å­˜åœ¨
        """
        return (
            self.index_path.exists() and
            self.metadata_path.exists() and
            self.config_path.exists()
        )
    
    def delete_collection(self) -> bool:
        """
        åˆ é™¤é›†åˆ
        
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            if self.index_path.exists():
                self.index_path.unlink()
            if self.metadata_path.exists():
                self.metadata_path.unlink()
            if self.config_path.exists():
                self.config_path.unlink()
            
            self.index = None
            self.metadata_store = {}
            self.id_to_idx = {}
            self.idx_to_id = {}
            self.next_idx = 0
            
            return True
        except Exception as e:
            print(f"åˆ é™¤ FAISS é›†åˆå¤±è´¥: {e}")
            return False
    
    def insert_vectors(
        self,
        vectors: List[np.ndarray],
        texts: List[str],
        metadatas: List[Dict[str, Any]],
        ids: Optional[List[str]] = None
    ) -> bool:
        """
        æ’å…¥å‘é‡
        
        Args:
            vectors: å‘é‡åˆ—è¡¨
            texts: æ–‡æœ¬åˆ—è¡¨
            metadatas: å…ƒæ•°æ®åˆ—è¡¨
            ids: ID åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
            
        Returns:
            æ˜¯å¦æ’å…¥æˆåŠŸ
        """
        try:
            if self.index is None:
                raise RuntimeError("ç´¢å¼•æœªåˆå§‹åŒ–")
            
            # éªŒè¯è¾“å…¥
            if not (len(vectors) == len(texts) == len(metadatas)):
                raise ValueError("å‘é‡ã€æ–‡æœ¬å’Œå…ƒæ•°æ®æ•°é‡å¿…é¡»ä¸€è‡´")
            
            # ç”Ÿæˆ IDï¼ˆå¦‚æœæœªæä¾›ï¼‰
            if ids is None:
                ids = [f"vec_{self.next_idx + i}" for i in range(len(vectors))]
            
            # è½¬æ¢å‘é‡ä¸º numpy æ•°ç»„
            vectors_array = np.array([v.astype('float32') for v in vectors])
            
            # æ·»åŠ åˆ° FAISS ç´¢å¼•
            self.index.add(vectors_array)
            
            # ä¿å­˜å…ƒæ•°æ®
            for i, (vec_id, text, metadata) in enumerate(zip(ids, texts, metadatas)):
                idx = self.next_idx + i
                self.metadata_store[idx] = {
                    "id": vec_id,
                    "text": text,
                    "metadata": metadata
                }
                self.id_to_idx[vec_id] = idx
                self.idx_to_id[idx] = vec_id
            
            self.next_idx += len(vectors)
            
            # ä¿å­˜åˆ°ç£ç›˜
            self._save()
            
            return True
        except Exception as e:
            print(f"æ’å…¥å‘é‡å¤±è´¥: {e}")
            return False
    
    def search(
        self,
        query_vector: np.ndarray,
        top_k: int = 5,
        filter_dict: Optional[Dict[str, Any]] = None
    ) -> List[VectorSearchResult]:
        """
        æœç´¢ç›¸ä¼¼å‘é‡
        
        Args:
            query_vector: æŸ¥è¯¢å‘é‡
            top_k: è¿”å›ç»“æœæ•°é‡
            filter_dict: è¿‡æ»¤æ¡ä»¶ï¼ˆå¯é€‰ï¼ŒFAISS ä¸æ”¯æŒåŸç”Ÿè¿‡æ»¤ï¼‰
            
        Returns:
            æœç´¢ç»“æœåˆ—è¡¨
        """
        try:
            if self.index is None or self.index.ntotal == 0:
                return []
            
            # è½¬æ¢æŸ¥è¯¢å‘é‡
            query_vector = query_vector.astype('float32').reshape(1, -1)
            
            # æœç´¢
            distances, indices = self.index.search(query_vector, min(top_k, self.index.ntotal))
            
            # æ„å»ºç»“æœ
            results = []
            for dist, idx in zip(distances[0], indices[0]):
                if idx == -1:  # FAISS è¿”å› -1 è¡¨ç¤ºæ— æ•ˆç»“æœ
                    continue
                
                idx = int(idx)
                if idx not in self.metadata_store:
                    continue
                
                metadata_entry = self.metadata_store[idx]
                
                # åº”ç”¨è¿‡æ»¤ï¼ˆå¦‚æœæä¾›ï¼‰
                if filter_dict:
                    skip = False
                    for key, value in filter_dict.items():
                        if metadata_entry["metadata"].get(key) != value:
                            skip = True
                            break
                    if skip:
                        continue
                
                result = VectorSearchResult(
                    id=metadata_entry["id"],
                    score=float(dist),
                    text=metadata_entry["text"],
                    metadata=metadata_entry["metadata"]
                )
                results.append(result)
            
            return results[:top_k]
        except Exception as e:
            print(f"æœç´¢å¤±è´¥: {e}")
            return []
    
    def delete_by_ids(self, ids: List[str]) -> bool:
        """
        æ ¹æ® ID åˆ é™¤å‘é‡
        
        æ³¨æ„ï¼šFAISS ä¸æ”¯æŒç›´æ¥åˆ é™¤ï¼Œéœ€è¦é‡å»ºç´¢å¼•
        
        Args:
            ids: ID åˆ—è¡¨
            
        Returns:
            æ˜¯å¦åˆ é™¤æˆåŠŸ
        """
        try:
            if self.index is None:
                return False
            
            # æ‰¾åˆ°è¦åˆ é™¤çš„ç´¢å¼•
            indices_to_delete = set()
            for vec_id in ids:
                if vec_id in self.id_to_idx:
                    indices_to_delete.add(self.id_to_idx[vec_id])
            
            if not indices_to_delete:
                return True
            
            # é‡å»ºç´¢å¼•ï¼ˆæ’é™¤è¦åˆ é™¤çš„å‘é‡ï¼‰
            new_index = faiss.IndexFlatL2(self.dimension)
            new_metadata_store = {}
            new_id_to_idx = {}
            new_idx_to_id = {}
            new_idx = 0
            
            # éå†ç°æœ‰å‘é‡
            for old_idx in range(self.index.ntotal):
                if old_idx in indices_to_delete:
                    continue
                
                # è·å–å‘é‡
                vector = self.index.reconstruct(old_idx)
                new_index.add(vector.reshape(1, -1))
                
                # å¤åˆ¶å…ƒæ•°æ®
                if old_idx in self.metadata_store:
                    metadata_entry = self.metadata_store[old_idx]
                    vec_id = metadata_entry["id"]
                    
                    new_metadata_store[new_idx] = metadata_entry
                    new_id_to_idx[vec_id] = new_idx
                    new_idx_to_id[new_idx] = vec_id
                    new_idx += 1
            
            # æ›´æ–°ç´¢å¼•å’Œå…ƒæ•°æ®
            self.index = new_index
            self.metadata_store = new_metadata_store
            self.id_to_idx = new_id_to_idx
            self.idx_to_id = new_idx_to_id
            self.next_idx = new_idx
            
            # ä¿å­˜
            self._save()
            
            return True
        except Exception as e:
            print(f"åˆ é™¤å‘é‡å¤±è´¥: {e}")
            return False
    
    def get_chunk_ids_by_document_id(self, document_id: str) -> List[str]:
        """
        æ ¹æ®æ–‡æ¡£ ID æŸ¥æ‰¾æ‰€æœ‰ç›¸å…³çš„å— ID
        
        Args:
            document_id: æ–‡æ¡£ ID
            
        Returns:
            å— ID åˆ—è¡¨
        """
        chunk_ids = []
        if self.index is None:
            return chunk_ids
        
        for old_idx in range(self.index.ntotal):
            if old_idx in self.metadata_store:
                metadata_entry = self.metadata_store[old_idx]
                # document_id å¯èƒ½åœ¨ metadata å­—å…¸ä¸­ï¼Œä¹Ÿå¯èƒ½åœ¨å— ID ä¸­ï¼ˆæ ¼å¼ï¼šdocument_id_chunk_Nï¼‰
                metadata = metadata_entry.get("metadata", {})
                vec_id = metadata_entry.get("id", "")
                
                # æ–¹æ³•1: ä» metadata ä¸­æŸ¥æ‰¾
                if metadata.get("document_id") == document_id:
                    chunk_ids.append(vec_id)
                # æ–¹æ³•2: ä»å— ID ä¸­æå–ï¼ˆæ ¼å¼ï¼šdocument_id_chunk_Nï¼‰
                elif vec_id.startswith(f"{document_id}_chunk_"):
                    chunk_ids.append(vec_id)
        
        return chunk_ids
    
    def get_vector_count(self) -> int:
        """
        è·å–å‘é‡æ•°é‡
        
        Returns:
            å‘é‡æ•°é‡
        """
        if self.index is None:
            return 0
        return self.index.ntotal
    
    def close(self):
        """å…³é—­è¿æ¥ï¼ˆä¿å­˜æ•°æ®ï¼‰"""
        self._save()
    
    def _save(self):
        """ä¿å­˜ç´¢å¼•å’Œå…ƒæ•°æ®åˆ°ç£ç›˜"""
        try:
            if self.index is not None:
                faiss.write_index(self.index, str(self.index_path))
            
            with open(self.metadata_path, 'wb') as f:
                pickle.dump({
                    "metadata_store": self.metadata_store,
                    "id_to_idx": self.id_to_idx,
                    "idx_to_id": self.idx_to_id,
                    "next_idx": self.next_idx
                }, f)
        except Exception as e:
            print(f"ä¿å­˜ FAISS æ•°æ®å¤±è´¥: {e}")
    
    def _load(self):
        """ä»ç£ç›˜åŠ è½½ç´¢å¼•å’Œå…ƒæ•°æ®"""
        try:
            # åŠ è½½ç´¢å¼•
            self.index = faiss.read_index(str(self.index_path))
            
            # åŠ è½½å…ƒæ•°æ®
            with open(self.metadata_path, 'rb') as f:
                data = pickle.load(f)
                self.metadata_store = data["metadata_store"]
                self.id_to_idx = data["id_to_idx"]
                self.idx_to_id = data["idx_to_id"]
                self.next_idx = data["next_idx"]
        except Exception as e:
            print(f"åŠ è½½ FAISS æ•°æ®å¤±è´¥: {e}")
            raise


if __name__ == "__main__":
    """æµ‹è¯• FAISS å­˜å‚¨"""
    print("=" * 60)
    print("FAISS å­˜å‚¨æµ‹è¯•")
    print("=" * 60)
    print()
    
    # åˆ›å»ºå­˜å‚¨
    store = FAISSStore(
        collection_name="test_collection",
        dimension=128,
        storage_dir="./test_faiss_data"
    )
    print("âœ… FAISS å­˜å‚¨åˆ›å»ºæˆåŠŸ")
    print(f"   {store.get_store_info()}")
    print()
    
    # æ’å…¥æµ‹è¯•å‘é‡
    print("ğŸ“¥ æ’å…¥æµ‹è¯•å‘é‡...")
    vectors = [np.random.rand(128).astype('float32') for _ in range(5)]
    texts = [f"æµ‹è¯•æ–‡æœ¬ {i}" for i in range(5)]
    metadatas = [{"index": i, "type": "test"} for i in range(5)]
    ids = [f"test_vec_{i}" for i in range(5)]
    
    success = store.insert_vectors(vectors, texts, metadatas, ids)
    print(f"   æ’å…¥{'æˆåŠŸ' if success else 'å¤±è´¥'}")
    print(f"   å‘é‡æ•°é‡: {store.get_vector_count()}")
    print()
    
    # æœç´¢æµ‹è¯•
    print("ğŸ” æœç´¢æµ‹è¯•...")
    query_vector = np.random.rand(128).astype('float32')
    results = store.search(query_vector, top_k=3)
    print(f"   æ‰¾åˆ° {len(results)} ä¸ªç»“æœ")
    for i, result in enumerate(results, 1):
        print(f"   {i}. ID: {result.id}, Score: {result.score:.4f}, Text: {result.text}")
    print()
    
    # åˆ é™¤æµ‹è¯•
    print("ğŸ—‘ï¸  åˆ é™¤æµ‹è¯•...")
    success = store.delete_by_ids(["test_vec_0"])
    print(f"   åˆ é™¤{'æˆåŠŸ' if success else 'å¤±è´¥'}")
    print(f"   å‰©ä½™å‘é‡: {store.get_vector_count()}")
    print()
    
    # æ¸…ç†
    store.delete_collection()
    print("âœ… æµ‹è¯•å®Œæˆï¼Œå·²æ¸…ç†æµ‹è¯•æ•°æ®")

