#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
æ–‡æ¡£å¤„ç†å™¨åŸºç±»
ä½¿ç”¨ç­–ç•¥æ¨¡å¼ï¼Œä¾¿äºæ‰©å±•ä¸åŒæ ¼å¼çš„æ–‡æ¡£å¤„ç†å™¨
"""

from abc import ABC, abstractmethod
from pathlib import Path
from typing import List, Dict, Any, Optional
from dataclasses import dataclass
from datetime import datetime


@dataclass
class DocumentChunk:
    """æ–‡æ¡£å—æ•°æ®ç±»"""
    text: str
    metadata: Dict[str, Any]
    chunk_id: str
    document_id: str
    chunk_index: int
    
    def __post_init__(self):
        """éªŒè¯æ•°æ®"""
        if not self.text or not self.text.strip():
            raise ValueError("æ–‡æœ¬å†…å®¹ä¸èƒ½ä¸ºç©º")
        if self.chunk_index < 0:
            raise ValueError("å—ç´¢å¼•å¿…é¡»å¤§äºç­‰äº 0")


@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç±»"""
    document_id: str
    file_name: str
    file_path: str
    file_size: int
    file_type: str
    upload_time: datetime
    chunks: List[DocumentChunk]
    metadata: Dict[str, Any]
    
    def get_total_chunks(self) -> int:
        """è·å–æ–‡æ¡£å—æ€»æ•°"""
        return len(self.chunks)
    
    def get_total_text_length(self) -> int:
        """è·å–æ–‡æ¡£æ€»æ–‡æœ¬é•¿åº¦"""
        return sum(len(chunk.text) for chunk in self.chunks)


class BaseDocumentProcessor(ABC):
    """
    æ–‡æ¡£å¤„ç†å™¨åŸºç±»
    
    æ‰€æœ‰æ–‡æ¡£å¤„ç†å™¨éƒ½åº”ç»§æ‰¿æ­¤ç±»å¹¶å®ç°æŠ½è±¡æ–¹æ³•
    """
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        åˆå§‹åŒ–æ–‡æ¡£å¤„ç†å™¨
        
        Args:
            chunk_size: æ–‡æœ¬åˆ†å—å¤§å°
            chunk_overlap: æ–‡æœ¬åˆ†å—é‡å å¤§å°
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self._validate_params()
    
    def _validate_params(self):
        """éªŒè¯å‚æ•°"""
        if self.chunk_size <= 0:
            raise ValueError("chunk_size å¿…é¡»å¤§äº 0")
        if self.chunk_overlap < 0:
            raise ValueError("chunk_overlap å¿…é¡»å¤§äºç­‰äº 0")
        if self.chunk_overlap >= self.chunk_size:
            raise ValueError("chunk_overlap å¿…é¡»å°äº chunk_size")
    
    @abstractmethod
    def extract_text(self, file_path: Path) -> str:
        """
        æå–æ–‡æ¡£æ–‡æœ¬
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æå–çš„æ–‡æœ¬å†…å®¹
        """
        pass
    
    @abstractmethod
    def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
        """
        æå–æ–‡æ¡£å…ƒæ•°æ®
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ–‡æ¡£å…ƒæ•°æ®å­—å…¸
        """
        pass
    
    @abstractmethod
    def supported_extensions(self) -> List[str]:
        """
        è·å–æ”¯æŒçš„æ–‡ä»¶æ‰©å±•å
        
        Returns:
            æ”¯æŒçš„æ‰©å±•ååˆ—è¡¨ï¼Œå¦‚ ['.pdf', '.txt']
        """
        pass
    
    def split_text(self, text: str) -> List[str]:
        """
        åˆ†å‰²æ–‡æœ¬ä¸ºå—
        
        Args:
            text: è¦åˆ†å‰²çš„æ–‡æœ¬
            
        Returns:
            æ–‡æœ¬å—åˆ—è¡¨
        """
        from src.utils.logger import logger
        
        if not text or not text.strip():
            return []
        
        chunks = []
        start = 0
        text_length = len(text)
        max_iterations = text_length // max(1, self.chunk_size - self.chunk_overlap) + 10  # é˜²æ­¢æ­»å¾ªç¯
        iteration = 0
        
        logger.info(f"âœ‚ï¸  æ–‡æœ¬åˆ†å‰²å‚æ•°: chunk_size={self.chunk_size}, chunk_overlap={self.chunk_overlap}, text_length={text_length}")
        
        while start < text_length:
            iteration += 1
            if iteration > max_iterations:
                logger.error(f"âŒ æ–‡æœ¬åˆ†å‰²å¯èƒ½é™·å…¥æ­»å¾ªç¯ï¼Œå·²å¤„ç† {len(chunks)} ä¸ªå—ï¼Œå½“å‰ä½ç½®: {start}/{text_length}")
                break
            
            # è®¡ç®—ç»“æŸä½ç½®
            end = min(start + self.chunk_size, text_length)
            
            # å¦‚æœä¸æ˜¯æœ€åä¸€å—ï¼Œå°è¯•åœ¨åˆé€‚çš„ä½ç½®åˆ†å‰²
            if end < text_length:
                # å¯»æ‰¾æœ€è¿‘çš„å¥å·ã€é—®å·ã€æ„Ÿå¹å·æˆ–æ¢è¡Œç¬¦
                split_chars = ['\n\n', '\n', 'ã€‚', 'ï¼', 'ï¼Ÿ', '.', '!', '?']
                best_split = end
                
                for char in split_chars:
                    pos = text.rfind(char, start, end)
                    if pos != -1:
                        best_split = pos + len(char)
                        break
                
                end = best_split
            
            # ç¡®ä¿ end > startï¼Œé˜²æ­¢ç©ºå—
            if end <= start:
                logger.warning(f"âš ï¸  æ£€æµ‹åˆ° end <= start ({end} <= {start})ï¼Œå¼ºåˆ¶æ¨è¿›")
                end = start + 1
            
            # æå–å—
            chunk = text[start:end].strip()
            if chunk:
                chunks.append(chunk)
            
            # æ›´æ–°èµ·å§‹ä½ç½®ï¼ˆè€ƒè™‘é‡å ï¼‰
            # ç¡®ä¿ start æ€»æ˜¯å¢åŠ çš„ï¼Œé˜²æ­¢æ­»å¾ªç¯
            new_start = end - self.chunk_overlap if end < text_length else text_length
            if new_start <= start:
                # å¦‚æœæ–°ä½ç½®æ²¡æœ‰æ¨è¿›ï¼Œè‡³å°‘æ¨è¿›1ä¸ªå­—ç¬¦
                new_start = start + 1
                logger.warning(f"âš ï¸  æ£€æµ‹åˆ°ä½ç½®æœªæ¨è¿›ï¼Œå¼ºåˆ¶æ¨è¿›åˆ° {new_start}")
            
            start = new_start
        
        logger.info(f"âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—ï¼Œè¿­ä»£æ¬¡æ•°: {iteration}")
        return chunks
    
    def process(self, file_path: Path, document_id: str) -> Document:
        """
        å¤„ç†æ–‡æ¡£
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            document_id: æ–‡æ¡£ ID
            
        Returns:
            å¤„ç†åçš„æ–‡æ¡£å¯¹è±¡
        """
        # éªŒè¯æ–‡ä»¶
        if not file_path.exists():
            raise FileNotFoundError(f"æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
        
        if not self.is_supported(file_path):
            raise ValueError(f"ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹: {file_path.suffix}")
        
        # æå–æ–‡æœ¬å’Œå…ƒæ•°æ®
        from src.utils.logger import logger
        logger.info(f"ğŸ“ å¼€å§‹æå–æ–‡æœ¬...")
        text = self.extract_text(file_path)
        logger.info(f"âœ… æ–‡æœ¬æå–å®Œæˆï¼Œé•¿åº¦: {len(text)} å­—ç¬¦")
        
        logger.info(f"ğŸ“‹ å¼€å§‹æå–å…ƒæ•°æ®...")
        metadata = self.extract_metadata(file_path)
        logger.info(f"âœ… å…ƒæ•°æ®æå–å®Œæˆ")
        
        # åˆ†å‰²æ–‡æœ¬
        logger.info(f"âœ‚ï¸  å¼€å§‹åˆ†å‰²æ–‡æœ¬ï¼Œæ–‡æœ¬é•¿åº¦: {len(text)} å­—ç¬¦")
        text_chunks = self.split_text(text)
        logger.info(f"âœ… æ–‡æœ¬åˆ†å‰²å®Œæˆï¼Œå…± {len(text_chunks)} ä¸ªæ–‡æœ¬å—")
        
        # åˆ›å»ºæ–‡æ¡£å—
        logger.info(f"ğŸ“¦ å¼€å§‹åˆ›å»ºæ–‡æ¡£å—...")
        chunks = []
        for i, chunk_text in enumerate(text_chunks):
            chunk = DocumentChunk(
                text=chunk_text,
                metadata={
                    "chunk_index": i,
                    "chunk_size": len(chunk_text),
                    "source_file": str(file_path),
                    **metadata
                },
                chunk_id=f"{document_id}_chunk_{i}",
                document_id=document_id,
                chunk_index=i
            )
            chunks.append(chunk)
        logger.info(f"âœ… æ–‡æ¡£å—åˆ›å»ºå®Œæˆï¼Œå…± {len(chunks)} ä¸ªå—")
        
        # åˆ›å»ºæ–‡æ¡£å¯¹è±¡
        logger.info(f"ğŸ“„ åˆ›å»ºæ–‡æ¡£å¯¹è±¡...")
        document = Document(
            document_id=document_id,
            file_name=file_path.name,
            file_path=str(file_path),
            file_size=file_path.stat().st_size,
            file_type=file_path.suffix,
            upload_time=datetime.now(),
            chunks=chunks,
            metadata=metadata
        )
        
        return document
    
    def is_supported(self, file_path: Path) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æ”¯æŒè¯¥æ–‡ä»¶
        """
        return file_path.suffix.lower() in self.supported_extensions()
    
    def get_processor_info(self) -> Dict[str, Any]:
        """
        è·å–å¤„ç†å™¨ä¿¡æ¯
        
        Returns:
            å¤„ç†å™¨ä¿¡æ¯å­—å…¸
        """
        return {
            "processor_name": self.__class__.__name__,
            "supported_extensions": self.supported_extensions(),
            "chunk_size": self.chunk_size,
            "chunk_overlap": self.chunk_overlap,
        }


class DocumentProcessorFactory:
    """æ–‡æ¡£å¤„ç†å™¨å·¥å‚ç±»"""
    
    _processors: Dict[str, BaseDocumentProcessor] = {}
    
    @classmethod
    def register_processor(cls, processor: BaseDocumentProcessor):
        """
        æ³¨å†Œæ–‡æ¡£å¤„ç†å™¨
        
        Args:
            processor: å¤„ç†å™¨å®ä¾‹
        """
        for ext in processor.supported_extensions():
            cls._processors[ext.lower()] = processor
    
    @classmethod
    def get_processor(cls, file_path: Path) -> Optional[BaseDocumentProcessor]:
        """
        æ ¹æ®æ–‡ä»¶æ‰©å±•åè·å–å¤„ç†å™¨
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            å¯¹åº”çš„å¤„ç†å™¨ï¼Œå¦‚æœä¸æ”¯æŒåˆ™è¿”å› None
        """
        ext = file_path.suffix.lower()
        return cls._processors.get(ext)
    
    @classmethod
    def get_supported_extensions(cls) -> List[str]:
        """
        è·å–æ‰€æœ‰æ”¯æŒçš„æ‰©å±•å
        
        Returns:
            æ”¯æŒçš„æ‰©å±•ååˆ—è¡¨
        """
        return list(cls._processors.keys())
    
    @classmethod
    def is_supported(cls, file_path: Path) -> bool:
        """
        æ£€æŸ¥æ–‡ä»¶æ˜¯å¦æ”¯æŒ
        
        Args:
            file_path: æ–‡ä»¶è·¯å¾„
            
        Returns:
            æ˜¯å¦æ”¯æŒè¯¥æ–‡ä»¶
        """
        return file_path.suffix.lower() in cls._processors


if __name__ == "__main__":
    """æµ‹è¯•æ–‡æ¡£å¤„ç†å™¨åŸºç±»"""
    print("=" * 60)
    print("æ–‡æ¡£å¤„ç†å™¨åŸºç±»æµ‹è¯•")
    print("=" * 60)
    print()
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„æµ‹è¯•å¤„ç†å™¨
    class TestProcessor(BaseDocumentProcessor):
        def extract_text(self, file_path: Path) -> str:
            return "æµ‹è¯•æ–‡æœ¬å†…å®¹"
        
        def extract_metadata(self, file_path: Path) -> Dict[str, Any]:
            return {"test_key": "test_value"}
        
        def supported_extensions(self) -> List[str]:
            return ['.txt']
    
    # æµ‹è¯•å¤„ç†å™¨
    processor = TestProcessor(chunk_size=10, chunk_overlap=2)
    print("âœ… å¤„ç†å™¨åˆ›å»ºæˆåŠŸ")
    print(f"   {processor.get_processor_info()}")
    print()
    
    # æµ‹è¯•æ–‡æœ¬åˆ†å‰²
    text = "è¿™æ˜¯ä¸€æ®µæµ‹è¯•æ–‡æœ¬ã€‚" * 10
    chunks = processor.split_text(text)
    print(f"âœ… æ–‡æœ¬åˆ†å‰²æµ‹è¯•")
    print(f"   åŸæ–‡é•¿åº¦: {len(text)}")
    print(f"   åˆ†å‰²å—æ•°: {len(chunks)}")
    print(f"   ç¬¬ä¸€å—: {chunks[0][:30]}...")
    print()
    
    print("âœ… åŸºç±»æµ‹è¯•å®Œæˆï¼")

