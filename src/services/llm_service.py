#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM æœåŠ¡
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œ API è°ƒç”¨
"""

from typing import Optional, List, Dict, Any
from pathlib import Path

from config.settings import settings


class LLMService:
    """LLM æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– LLM æœåŠ¡"""
        self.model_type = settings.LLM_MODEL_TYPE
        self.model_name = settings.LLM_MODEL_NAME
        self.temperature = settings.LLM_TEMPERATURE
        self.max_tokens = settings.LLM_MAX_TOKENS
        self.top_p = settings.LLM_TOP_P
        
        self.model = None
        self.tokenizer = None
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.model_type == "local":
            self._load_local_model()
        elif self.model_type == "api":
            self._setup_api_client()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _load_local_model(self):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            model_path = settings.get_llm_model_path()
            if not model_path or not model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            
            print(f"ğŸ“¦ åŠ è½½æœ¬åœ° LLM æ¨¡å‹: {model_path}")
            
            # åŠ è½½ tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(model_path),
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = AutoModelForCausalLM.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            
            if device == "cpu":
                self.model = self.model.to(device)
            
            print(f"âœ… LLM æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {device})")
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æœ¬åœ° LLM æ¨¡å‹å¤±è´¥: {e}")
    
    def _setup_api_client(self):
        """è®¾ç½® API å®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            
            self.model = OpenAI(
                api_key=settings.LLM_API_KEY,
                base_url=settings.LLM_API_BASE
            )
            print(f"âœ… LLM API å®¢æˆ·ç«¯è®¾ç½®æˆåŠŸ")
            
        except Exception as e:
            raise RuntimeError(f"è®¾ç½® LLM API å®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if not prompt or not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        temp = temperature if temperature is not None else self.temperature
        max_tok = max_tokens if max_tokens is not None else self.max_tokens
        
        if self.model_type == "local":
            return self._generate_with_local_model(prompt, system_prompt, temp, max_tok)
        else:
            return self._generate_with_api(prompt, system_prompt, temp, max_tok)
    
    def _generate_with_local_model(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        try:
            import torch
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Tokenize
            inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=self.top_p,
                    do_sample=temperature > 0
                )
            
            # è§£ç 
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            raise RuntimeError(f"æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
    
    def _generate_with_api(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> str:
        """ä½¿ç”¨ API ç”Ÿæˆ"""
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            response = self.model.chat.completions.create(
                model=self.model_name,
                messages=messages,
                temperature=temperature,
                max_tokens=max_tokens,
                top_p=self.top_p
            )
            
            return response.choices[0].message.content.strip()
            
        except Exception as e:
            raise RuntimeError(f"API ç”Ÿæˆå¤±è´¥: {e}")
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p
        }


# å…¨å±€å•ä¾‹
_llm_service: Optional[LLMService] = None


def get_llm_service() -> LLMService:
    """è·å– LLM æœåŠ¡å•ä¾‹"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service


if __name__ == "__main__":
    """æµ‹è¯• LLM æœåŠ¡"""
    print("=" * 60)
    print("LLM æœåŠ¡æµ‹è¯•")
    print("=" * 60)
    print()
    
    try:
        # åˆ›å»ºæœåŠ¡
        service = get_llm_service()
        print(f"âœ… æœåŠ¡åˆ›å»ºæˆåŠŸ")
        print(f"   {service.get_model_info()}")
        print()
        
        # æµ‹è¯•ç”Ÿæˆ
        print("ğŸ“ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚"
        response = service.generate(prompt)
        print(f"   æç¤ºè¯: {prompt}")
        print(f"   å›å¤: {response}")
        print()
        
        print("âœ… LLM æœåŠ¡æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

