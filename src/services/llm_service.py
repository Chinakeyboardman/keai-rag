#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
LLM æœåŠ¡
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œ API è°ƒç”¨
"""

from typing import Optional, List, Dict, Any, Iterator
from pathlib import Path

from config.settings import settings


class LLMService:
    """LLM æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– LLM æœåŠ¡"""
        self.model_type = settings.LLM_MODEL_TYPE
        self.model_name = settings.LLM_MODEL_NAME
        self.temperature = settings.LLM_TEMPERATURE
        self.max_tokens = settings.LLM_MAX_TOKENS
        self.top_p = settings.LLM_TOP_P
        
        self.model = None
        self.tokenizer = None
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.model_type == "local":
            self._load_local_model()
        elif self.model_type == "api":
            self._setup_api_client()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _load_local_model(self):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        try:
            from transformers import AutoModelForCausalLM, AutoTokenizer
            import torch
            
            model_path = settings.get_llm_model_path()
            if not model_path or not model_path.exists():
                raise FileNotFoundError(f"æ¨¡å‹è·¯å¾„ä¸å­˜åœ¨: {model_path}")
            
            print(f"ğŸ“¦ åŠ è½½æœ¬åœ° LLM æ¨¡å‹: {model_path}")
            
            # åŠ è½½ tokenizer
            self.tokenizer = AutoTokenizer.from_pretrained(
                str(model_path),
                trust_remote_code=True
            )
            
            # åŠ è½½æ¨¡å‹
            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.model = AutoModelForCausalLM.from_pretrained(
                str(model_path),
                trust_remote_code=True,
                torch_dtype=torch.float16 if device == "cuda" else torch.float32,
                device_map="auto" if device == "cuda" else None
            )
            
            if device == "cpu":
                self.model = self.model.to(device)
            
            print(f"âœ… LLM æ¨¡å‹åŠ è½½æˆåŠŸ (è®¾å¤‡: {device})")
            
        except Exception as e:
            raise RuntimeError(f"åŠ è½½æœ¬åœ° LLM æ¨¡å‹å¤±è´¥: {e}")
    
    def _setup_api_client(self):
        """è®¾ç½® API å®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            
            self.model = OpenAI(
                api_key=settings.LLM_API_KEY,
                base_url=settings.LLM_API_BASE
            )
            print(f"âœ… LLM API å®¢æˆ·ç«¯è®¾ç½®æˆåŠŸ")
            
        except Exception as e:
            raise RuntimeError(f"è®¾ç½® LLM API å®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> str:
        """
        ç”Ÿæˆæ–‡æœ¬
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            
        Returns:
            ç”Ÿæˆçš„æ–‡æœ¬
        """
        if not prompt or not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        temp = temperature if temperature is not None else self.temperature
        max_tok = max_tokens if max_tokens is not None else self.max_tokens
        
        if self.model_type == "local":
            return self._generate_with_local_model(prompt, system_prompt, temp, max_tok)
        else:
            return self._generate_with_api(prompt, system_prompt, temp, max_tok)
    
    def _generate_with_local_model(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> str:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹ç”Ÿæˆ"""
        try:
            import torch
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Tokenize
            inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # ç”Ÿæˆ
            with torch.no_grad():
                outputs = self.model.generate(
                    **inputs,
                    max_new_tokens=max_tokens,
                    temperature=temperature,
                    top_p=self.top_p,
                    do_sample=temperature > 0
                )
            
            # è§£ç 
            generated_text = self.tokenizer.decode(
                outputs[0][inputs['input_ids'].shape[1]:],
                skip_special_tokens=True
            )
            
            return generated_text.strip()
            
        except Exception as e:
            raise RuntimeError(f"æœ¬åœ°æ¨¡å‹ç”Ÿæˆå¤±è´¥: {e}")
    
    def _generate_with_api(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> str:
        """ä½¿ç”¨ API ç”Ÿæˆ"""
        from src.utils.logger import logger
        import time
        import requests
        
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            logger.info(f"ğŸ“ è°ƒç”¨ LLM API: {self.model_name}")
            logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            logger.info(f"âš™ï¸  å‚æ•°: temperature={temperature}, max_tokens={max_tokens}")
            logger.info(f"ğŸŒ API Base URL: {settings.LLM_API_BASE}")
            
            # æ£€æŸ¥ API è¿æ¥ï¼ˆä»…å¯¹æœ¬åœ° Ollama è¿›è¡Œä¸¥æ ¼æ£€æŸ¥ï¼‰
            base_url = settings.LLM_API_BASE.rstrip('/v1').rstrip('/')
            if 'localhost' in base_url or '127.0.0.1' in base_url:
                # æ£€æŸ¥ Ollama è¿æ¥
                health_url = base_url.replace('/v1', '') + '/api/tags'
                logger.info(f"ğŸ” æ£€æŸ¥ Ollama è¿æ¥: {health_url}")
                try:
                    resp = requests.get(health_url, timeout=10)
                    if resp.status_code == 200:
                        logger.info(f"âœ… Ollama æœåŠ¡è¿æ¥æ­£å¸¸")
                    else:
                        logger.error(f"âŒ Ollama æœåŠ¡å“åº”å¼‚å¸¸: {resp.status_code}")
                        raise RuntimeError(f"Ollama æœåŠ¡ä¸å¯ç”¨ï¼ŒHTTP çŠ¶æ€ç : {resp.status_code}ã€‚è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼šollama serve")
                except requests.exceptions.Timeout:
                    logger.error(f"âŒ Ollama æœåŠ¡è¿æ¥è¶…æ—¶ï¼ˆ10ç§’ï¼‰")
                    raise RuntimeError("Ollama æœåŠ¡è¿æ¥è¶…æ—¶ã€‚è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼šollama serve")
                except requests.exceptions.ConnectionError as e:
                    logger.error(f"âŒ æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡: {e}")
                    raise RuntimeError(f"æ— æ³•è¿æ¥åˆ° Ollama æœåŠ¡ã€‚è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼šollama serve")
                except Exception as e:
                    logger.error(f"âŒ Ollama è¿æ¥æ£€æŸ¥å¤±è´¥: {e}")
                    raise RuntimeError(f"Ollama è¿æ¥æ£€æŸ¥å¤±è´¥: {e}ã€‚è¯·ç¡®ä¿ Ollama æœåŠ¡æ­£åœ¨è¿è¡Œï¼šollama serve")
            
            start_time = time.time()
            logger.info(f"â³ å¼€å§‹å‘é€è¯·æ±‚åˆ° LLM API...")
            
            try:
                response = self.model.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=self.top_p,
                    timeout=120.0  # 120ç§’è¶…æ—¶
                )
            except Exception as api_error:
                elapsed_time = time.time() - start_time
                logger.error(f"âŒ LLM API è°ƒç”¨å¼‚å¸¸ï¼ˆè€—æ—¶ {elapsed_time:.2f} ç§’ï¼‰: {type(api_error).__name__}: {api_error}")
                raise
            
            elapsed_time = time.time() - start_time
            
            if not response.choices or not response.choices[0].message.content:
                logger.error(f"âŒ LLM API è¿”å›ç©ºå“åº”")
                raise RuntimeError("LLM API è¿”å›ç©ºå“åº”")
            
            answer = response.choices[0].message.content.strip()
            logger.info(f"âœ… LLM API è°ƒç”¨æˆåŠŸï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’ï¼Œå“åº”é•¿åº¦: {len(answer)} å­—ç¬¦")
            
            return answer
            
        except RuntimeError:
            raise
        except Exception as e:
            logger.error(f"âŒ LLM API è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            raise RuntimeError(f"API ç”Ÿæˆå¤±è´¥: {e}")
    
    def generate_stream(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        temperature: Optional[float] = None,
        max_tokens: Optional[int] = None
    ) -> Iterator[str]:
        """
        æµå¼ç”Ÿæˆæ–‡æœ¬ï¼ˆé€tokenè¿”å›ï¼‰
        
        Args:
            prompt: ç”¨æˆ·æç¤ºè¯
            system_prompt: ç³»ç»Ÿæç¤ºè¯
            temperature: æ¸©åº¦å‚æ•°
            max_tokens: æœ€å¤§ token æ•°
            
        Yields:
            ç”Ÿæˆçš„æ–‡æœ¬ç‰‡æ®µ
        """
        if not prompt or not prompt.strip():
            raise ValueError("æç¤ºè¯ä¸èƒ½ä¸ºç©º")
        
        temp = temperature if temperature is not None else self.temperature
        max_tok = max_tokens if max_tokens is not None else self.max_tokens
        
        if self.model_type == "local":
            yield from self._generate_stream_with_local_model(prompt, system_prompt, temp, max_tok)
        else:
            yield from self._generate_stream_with_api(prompt, system_prompt, temp, max_tok)
    
    def _generate_stream_with_local_model(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> Iterator[str]:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹æµå¼ç”Ÿæˆ"""
        try:
            import torch
            from transformers import TextIteratorStreamer
            from threading import Thread
            
            # æ„å»ºæ¶ˆæ¯
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            # åº”ç”¨èŠå¤©æ¨¡æ¿
            text = self.tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=True
            )
            
            # Tokenize
            inputs = self.tokenizer([text], return_tensors="pt").to(self.model.device)
            
            # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
            streamer = TextIteratorStreamer(
                self.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            # åœ¨å•ç‹¬çº¿ç¨‹ä¸­ç”Ÿæˆ
            generation_kwargs = {
                **inputs,
                "max_new_tokens": max_tokens,
                "temperature": temperature,
                "top_p": self.top_p,
                "do_sample": temperature > 0,
                "streamer": streamer
            }
            
            thread = Thread(target=self.model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # æµå¼è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
            for text in streamer:
                if text:
                    yield text
                    
        except Exception as e:
            raise RuntimeError(f"æœ¬åœ°æ¨¡å‹æµå¼ç”Ÿæˆå¤±è´¥: {e}")
    
    def _generate_stream_with_api(
        self,
        prompt: str,
        system_prompt: Optional[str],
        temperature: float,
        max_tokens: int
    ) -> Iterator[str]:
        """ä½¿ç”¨ API æµå¼ç”Ÿæˆ"""
        from src.utils.logger import logger
        import time
        
        try:
            messages = []
            if system_prompt:
                messages.append({"role": "system", "content": system_prompt})
            messages.append({"role": "user", "content": prompt})
            
            logger.info(f"ğŸ“ æµå¼è°ƒç”¨ LLM API: {self.model_name}")
            logger.info(f"ğŸ“ æç¤ºè¯é•¿åº¦: {len(prompt)} å­—ç¬¦")
            
            start_time = time.time()
            
            try:
                # ä½¿ç”¨æµå¼API
                stream = self.model.chat.completions.create(
                    model=self.model_name,
                    messages=messages,
                    temperature=temperature,
                    max_tokens=max_tokens,
                    top_p=self.top_p,
                    stream=True,  # å¯ç”¨æµå¼è¾“å‡º
                    timeout=120.0
                )
                
                # é€å—è¿”å›ç”Ÿæˆçš„æ–‡æœ¬
                for chunk in stream:
                    if chunk.choices and len(chunk.choices) > 0:
                        delta = chunk.choices[0].delta
                        if delta and delta.content:
                            yield delta.content
                
                elapsed_time = time.time() - start_time
                logger.info(f"âœ… LLM API æµå¼è°ƒç”¨æˆåŠŸï¼Œè€—æ—¶ {elapsed_time:.2f} ç§’")
                
            except Exception as api_error:
                elapsed_time = time.time() - start_time
                logger.error(f"âŒ LLM API æµå¼è°ƒç”¨å¼‚å¸¸ï¼ˆè€—æ—¶ {elapsed_time:.2f} ç§’ï¼‰: {type(api_error).__name__}: {api_error}")
                raise
                
        except RuntimeError:
            raise
        except Exception as e:
            logger.error(f"âŒ LLM API æµå¼è°ƒç”¨å¤±è´¥: {type(e).__name__}: {e}", exc_info=True)
            raise RuntimeError(f"API æµå¼ç”Ÿæˆå¤±è´¥: {e}")
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "top_p": self.top_p
        }


# å…¨å±€å•ä¾‹
_llm_service: Optional[LLMService] = None


def get_llm_service() -> LLMService:
    """è·å– LLM æœåŠ¡å•ä¾‹"""
    global _llm_service
    if _llm_service is None:
        _llm_service = LLMService()
    return _llm_service


if __name__ == "__main__":
    """æµ‹è¯• LLM æœåŠ¡"""
    print("=" * 60)
    print("LLM æœåŠ¡æµ‹è¯•")
    print("=" * 60)
    print()
    
    try:
        # åˆ›å»ºæœåŠ¡
        service = get_llm_service()
        print(f"âœ… æœåŠ¡åˆ›å»ºæˆåŠŸ")
        print(f"   {service.get_model_info()}")
        print()
        
        # æµ‹è¯•ç”Ÿæˆ
        print("ğŸ“ æµ‹è¯•æ–‡æœ¬ç”Ÿæˆ...")
        prompt = "è¯·ç”¨ä¸€å¥è¯è§£é‡Šä»€ä¹ˆæ˜¯äººå·¥æ™ºèƒ½ã€‚"
        response = service.generate(prompt)
        print(f"   æç¤ºè¯: {prompt}")
        print(f"   å›å¤: {response}")
        print()
        
        print("âœ… LLM æœåŠ¡æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

