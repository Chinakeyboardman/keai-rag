#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Embedding æœåŠ¡
æ”¯æŒæœ¬åœ°æ¨¡å‹å’Œ API è°ƒç”¨
"""

from typing import List, Optional
import numpy as np
from pathlib import Path

from config.settings import settings
from src.utils.logger import logger


class EmbeddingService:
    """Embedding æœåŠ¡ç±»"""
    
    def __init__(self):
        """åˆå§‹åŒ– Embedding æœåŠ¡"""
        self.model_type = settings.EMBEDDING_MODEL_TYPE
        self.model_name = settings.EMBEDDING_MODEL_NAME
        self.dimension = settings.VECTOR_DIMENSION
        self.batch_size = settings.EMBEDDING_BATCH_SIZE
        
        self.model = None
        self._initialize_model()
    
    def _initialize_model(self):
        """åˆå§‹åŒ–æ¨¡å‹"""
        if self.model_type == "local":
            self._load_local_model()
        elif self.model_type == "api":
            self._setup_api_client()
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡å‹ç±»å‹: {self.model_type}")
    
    def _load_local_model(self):
        """åŠ è½½æœ¬åœ°æ¨¡å‹"""
        from sentence_transformers import SentenceTransformer
        import os
        
        # é¦–å…ˆå°è¯•ä½¿ç”¨é…ç½®çš„æ¨¡å‹è·¯å¾„
        model_path = settings.get_embedding_model_path()
        if model_path and model_path.exists():
            logger.info(f"ğŸ“¦ åŠ è½½æœ¬åœ° Embedding æ¨¡å‹: {model_path}")
            try:
                # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶ç¦»çº¿æ¨¡å¼
                os.environ['HF_HUB_OFFLINE'] = '1'
                self.model = SentenceTransformer(str(model_path), local_files_only=True)
                logger.info(f"âœ… Embedding æ¨¡å‹åŠ è½½æˆåŠŸ")
                return
            except Exception as e:
                logger.warning(f"âš ï¸  ä½¿ç”¨é…ç½®è·¯å¾„åŠ è½½å¤±è´¥: {e}")
        
        # å°è¯•ä» HuggingFace ç¼“å­˜åŠ è½½
        cache_dir = os.path.expanduser("~/.cache/huggingface/hub")
        model_cache_name = f"models--{self.model_name.replace('/', '--')}"
        model_cache_path = os.path.join(cache_dir, model_cache_name, "snapshots")
        
        if os.path.exists(model_cache_path):
            # æŸ¥æ‰¾æœ€æ–°çš„å¿«ç…§
            snapshots = [d for d in os.listdir(model_cache_path) if os.path.isdir(os.path.join(model_cache_path, d))]
            if snapshots:
                latest_snapshot = os.path.join(model_cache_path, snapshots[0])
                logger.info(f"ğŸ“¦ ä»ç¼“å­˜åŠ è½½ Embedding æ¨¡å‹: {latest_snapshot}")
                try:
                    os.environ['HF_HUB_OFFLINE'] = '1'
                    self.model = SentenceTransformer(latest_snapshot, local_files_only=True)
                    logger.info(f"âœ… ä»ç¼“å­˜åŠ è½½æ¨¡å‹æˆåŠŸ")
                    return
                except Exception as e:
                    logger.warning(f"âš ï¸  ä»ç¼“å­˜åŠ è½½å¤±è´¥: {e}")
        
        # æœ€åå°è¯•ä½¿ç”¨æ¨¡å‹åç§°åŠ è½½ï¼ˆä¼šå°è¯•è¿æ¥ç½‘ç»œï¼‰
        logger.info(f"ğŸ“¦ å°è¯•åŠ è½½ Embedding æ¨¡å‹: {self.model_name}")
        try:
            # å…ˆå°è¯•ç¦»çº¿æ¨¡å¼
            os.environ['HF_HUB_OFFLINE'] = '1'
            self.model = SentenceTransformer(self.model_name, local_files_only=True)
            logger.info(f"âœ… æ¨¡å‹åŠ è½½æˆåŠŸï¼ˆç¦»çº¿æ¨¡å¼ï¼‰")
            return
        except Exception as e:
            logger.warning(f"âš ï¸  ç¦»çº¿åŠ è½½å¤±è´¥: {e}")
            # å¦‚æœç¦»çº¿å¤±è´¥ï¼Œå°è¯•åœ¨çº¿ä¸‹è½½ï¼ˆå¯èƒ½é‡åˆ°é€Ÿç‡é™åˆ¶ï¼‰
            try:
                os.environ.pop('HF_HUB_OFFLINE', None)  # ç§»é™¤ç¦»çº¿æ¨¡å¼
                logger.info(f"ğŸ“¦ å°è¯•åœ¨çº¿ä¸‹è½½æ¨¡å‹...")
                self.model = SentenceTransformer(self.model_name)
                logger.info(f"âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½æˆåŠŸ")
                return
            except Exception as e:
                error_msg = (
                    f"åŠ è½½ Embedding æ¨¡å‹å¤±è´¥: {e}\n"
                    f"æç¤º: å¦‚æœé‡åˆ°é€Ÿç‡é™åˆ¶ï¼Œè¯·:\n"
                    f"  1. ç­‰å¾…ä¸€æ®µæ—¶é—´åé‡è¯•\n"
                    f"  2. è®¾ç½® HF_TOKEN ç¯å¢ƒå˜é‡\n"
                    f"  3. æˆ–æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹åˆ°ç¼“å­˜ç›®å½•"
                )
                logger.error(error_msg)
                raise RuntimeError(error_msg)
    
    def _setup_api_client(self):
        """è®¾ç½® API å®¢æˆ·ç«¯"""
        try:
            from openai import OpenAI
            
            self.model = OpenAI(
                api_key=settings.EMBEDDING_API_KEY,
                base_url=settings.EMBEDDING_API_BASE
            )
            print(f"âœ… Embedding API å®¢æˆ·ç«¯è®¾ç½®æˆåŠŸ")
            
        except Exception as e:
            raise RuntimeError(f"è®¾ç½® Embedding API å®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    def embed_text(self, text: str) -> np.ndarray:
        """
        å¯¹å•ä¸ªæ–‡æœ¬è¿›è¡Œå‘é‡åŒ–
        
        Args:
            text: è¾“å…¥æ–‡æœ¬
            
        Returns:
            æ–‡æœ¬å‘é‡
        """
        if not text or not text.strip():
            raise ValueError("æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        return self.embed_texts([text])[0]
    
    def embed_texts(self, texts: List[str]) -> List[np.ndarray]:
        """
        å¯¹å¤šä¸ªæ–‡æœ¬è¿›è¡Œæ‰¹é‡å‘é‡åŒ–
        
        Args:
            texts: æ–‡æœ¬åˆ—è¡¨
            
        Returns:
            å‘é‡åˆ—è¡¨
        """
        if not texts:
            return []
        
        # è¿‡æ»¤ç©ºæ–‡æœ¬
        valid_texts = [t for t in texts if t and t.strip()]
        if not valid_texts:
            raise ValueError("æ²¡æœ‰æœ‰æ•ˆçš„æ–‡æœ¬")
        
        if self.model_type == "local":
            return self._embed_with_local_model(valid_texts)
        else:
            return self._embed_with_api(valid_texts)
    
    def _embed_with_local_model(self, texts: List[str]) -> List[np.ndarray]:
        """ä½¿ç”¨æœ¬åœ°æ¨¡å‹è¿›è¡Œå‘é‡åŒ–"""
        try:
            if self.model is None:
                raise RuntimeError("Embedding æ¨¡å‹æœªåˆå§‹åŒ–")
            
            logger.info(f"ğŸ”„ å¼€å§‹å‘é‡åŒ– {len(texts)} ä¸ªæ–‡æœ¬å—ï¼ˆæ‰¹é‡å¤§å°: {self.batch_size}ï¼‰...")
            
            # æ‰¹é‡ç¼–ç 
            embeddings = self.model.encode(
                texts,
                batch_size=self.batch_size,
                show_progress_bar=True,  # æ˜¾ç¤ºè¿›åº¦æ¡
                convert_to_numpy=True
            )
            
            logger.info(f"âœ… å‘é‡åŒ–å®Œæˆï¼Œç”Ÿæˆ {len(embeddings)} ä¸ªå‘é‡")
            
            # è½¬æ¢ä¸ºåˆ—è¡¨
            return [emb.astype('float32') for emb in embeddings]
            
        except Exception as e:
            logger.error(f"âŒ æœ¬åœ°æ¨¡å‹å‘é‡åŒ–å¤±è´¥: {e}", exc_info=True)
            raise RuntimeError(f"æœ¬åœ°æ¨¡å‹å‘é‡åŒ–å¤±è´¥: {e}")
    
    def _embed_with_api(self, texts: List[str]) -> List[np.ndarray]:
        """ä½¿ç”¨ API è¿›è¡Œå‘é‡åŒ–"""
        try:
            embeddings = []
            
            # åˆ†æ‰¹å¤„ç†
            for i in range(0, len(texts), self.batch_size):
                batch = texts[i:i + self.batch_size]
                
                response = self.model.embeddings.create(
                    model=self.model_name,
                    input=batch,
                    dimensions=self.dimension
                )
                
                batch_embeddings = [
                    np.array(item.embedding, dtype='float32')
                    for item in response.data
                ]
                embeddings.extend(batch_embeddings)
            
            return embeddings
            
        except Exception as e:
            raise RuntimeError(f"API å‘é‡åŒ–å¤±è´¥: {e}")
    
    def get_dimension(self) -> int:
        """è·å–å‘é‡ç»´åº¦"""
        return self.dimension
    
    def get_model_info(self) -> dict:
        """è·å–æ¨¡å‹ä¿¡æ¯"""
        return {
            "model_type": self.model_type,
            "model_name": self.model_name,
            "dimension": self.dimension,
            "batch_size": self.batch_size
        }


# å…¨å±€å•ä¾‹
_embedding_service: Optional[EmbeddingService] = None


def get_embedding_service() -> EmbeddingService:
    """è·å– Embedding æœåŠ¡å•ä¾‹"""
    global _embedding_service
    if _embedding_service is None:
        _embedding_service = EmbeddingService()
    return _embedding_service


if __name__ == "__main__":
    """æµ‹è¯• Embedding æœåŠ¡"""
    print("=" * 60)
    print("Embedding æœåŠ¡æµ‹è¯•")
    print("=" * 60)
    print()
    
    try:
        # åˆ›å»ºæœåŠ¡
        service = get_embedding_service()
        print(f"âœ… æœåŠ¡åˆ›å»ºæˆåŠŸ")
        print(f"   {service.get_model_info()}")
        print()
        
        # æµ‹è¯•å•ä¸ªæ–‡æœ¬
        print("ğŸ“ æµ‹è¯•å•ä¸ªæ–‡æœ¬å‘é‡åŒ–...")
        text = "è¿™æ˜¯ä¸€ä¸ªæµ‹è¯•æ–‡æœ¬"
        vector = service.embed_text(text)
        print(f"   æ–‡æœ¬: {text}")
        print(f"   å‘é‡ç»´åº¦: {vector.shape}")
        print(f"   å‘é‡å‰5ç»´: {vector[:5]}")
        print()
        
        # æµ‹è¯•æ‰¹é‡æ–‡æœ¬
        print("ğŸ“ æµ‹è¯•æ‰¹é‡æ–‡æœ¬å‘é‡åŒ–...")
        texts = [
            "äººå·¥æ™ºèƒ½æ˜¯è®¡ç®—æœºç§‘å­¦çš„ä¸€ä¸ªåˆ†æ”¯",
            "æœºå™¨å­¦ä¹ æ˜¯å®ç°äººå·¥æ™ºèƒ½çš„ä¸€ç§æ–¹æ³•",
            "æ·±åº¦å­¦ä¹ æ˜¯æœºå™¨å­¦ä¹ çš„ä¸€ä¸ªå­é¢†åŸŸ"
        ]
        vectors = service.embed_texts(texts)
        print(f"   æ–‡æœ¬æ•°é‡: {len(texts)}")
        print(f"   å‘é‡æ•°é‡: {len(vectors)}")
        print(f"   æ¯ä¸ªå‘é‡ç»´åº¦: {vectors[0].shape}")
        print()
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        print("ğŸ” è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦...")
        from numpy.linalg import norm
        
        def cosine_similarity(a, b):
            return np.dot(a, b) / (norm(a) * norm(b))
        
        sim_01 = cosine_similarity(vectors[0], vectors[1])
        sim_02 = cosine_similarity(vectors[0], vectors[2])
        sim_12 = cosine_similarity(vectors[1], vectors[2])
        
        print(f"   æ–‡æœ¬0 vs æ–‡æœ¬1: {sim_01:.4f}")
        print(f"   æ–‡æœ¬0 vs æ–‡æœ¬2: {sim_02:.4f}")
        print(f"   æ–‡æœ¬1 vs æ–‡æœ¬2: {sim_12:.4f}")
        print()
        
        print("âœ… Embedding æœåŠ¡æµ‹è¯•å®Œæˆï¼")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

